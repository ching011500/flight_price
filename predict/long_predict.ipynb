{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é æ¸¬åƒ¹æ ¼åŠå€é–“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹æ¸¬è©¦é›† MSE = 0.0319, R2 = 0.8960\n",
      "ğŸ”¹æ¸¬è©¦é›† MSE = 0.0165, R2 = 0.8761\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_CDG.csvï¼ˆå…± 1576 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_FRA.csvï¼ˆå…± 2120 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_JFK.csvï¼ˆå…± 1221 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_LAX.csvï¼ˆå…± 2293 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_LHR.csvï¼ˆå…± 2644 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_SYD.csvï¼ˆå…± 1986 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/eco_ZRH.csvï¼ˆå…± 1367 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_CDG.csvï¼ˆå…± 1271 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_FRA.csvï¼ˆå…± 1675 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_JFK.csvï¼ˆå…± 1235 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_LAX.csvï¼ˆå…± 2240 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_LHR.csvï¼ˆå…± 2003 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_SYD.csvï¼ˆå…± 1603 ç­†ï¼‰\n",
      "âœ… é æ¸¬çµæœå·²å­˜ï¼špredict_data/long/biz_ZRH.csvï¼ˆå…± 1096 ç­†ï¼‰\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --------------------------\n",
    "# 1. è®€å–è³‡æ–™\n",
    "# --------------------------\n",
    "df = pd.read_csv('/Users/yuchingchen/Documents/å°ˆé¡Œ/merge_and_cleaned/final_data/long_flight.csv')\n",
    "\n",
    "# é å…ˆåˆ†ç¶“æ¿Ÿè‰™ã€å•†å‹™è‰™\n",
    "eco_raw = df[df['è‰™ç­‰ï¼ˆä¸»èˆªæ®µï¼‰'] == 'ç¶“æ¿Ÿè‰™'].copy()\n",
    "biz_raw = df[df['è‰™ç­‰ï¼ˆä¸»èˆªæ®µï¼‰'] == 'å•†å‹™è‰™'].copy()\n",
    "\n",
    "# --------------------------\n",
    "# 2. æ¬„ä½å®šç¾©\n",
    "# --------------------------\n",
    "cat_cols = [\n",
    "    'å‡ºç™¼æ™‚æ®µ', 'å‡ºç™¼æ©Ÿå ´ä»£è™Ÿ', 'æŠµé”æ™‚æ®µ', 'èˆªç©ºè¯ç›Ÿçµ„åˆ', 'èˆªç©ºå…¬å¸çµ„åˆ',\n",
    "    'èˆªç©ºè¯ç›Ÿ', 'åœé ç«™æ•¸é‡', 'æ©Ÿå‹åˆ†é¡', 'å‡æœŸ', 'é£›è¡Œæ™‚é–“å…©æ®µåˆ†é¡', 'æ˜¯å¦ç‚ºå¹³æ—¥'\n",
    "]\n",
    "num_cols = ['åœç•™æ™‚é–“_åˆ†é˜', 'å¯¦éš›é£›è¡Œæ™‚é–“_åˆ†é˜', 'ç¶“æ¿ŸæŒ‡æ¨™', 'æ©Ÿå ´æŒ‡æ¨™', 'competing_flights']\n",
    "group_col = 'æŠµé”æ©Ÿå ´ä»£è™Ÿ'\n",
    "log_col = 'å¹³å‡åƒ¹æ ¼_log'\n",
    "true_col = 'å¹³å‡åƒ¹æ ¼'\n",
    "\n",
    "# --------------------------\n",
    "# 3. ç·¨ç¢¼èˆ‡æ¨™æº–åŒ–\n",
    "# --------------------------\n",
    "def encode_and_scale(df_raw):\n",
    "    cols = cat_cols + num_cols + [log_col, true_col]\n",
    "    df_sub = df_raw[cols]\n",
    "    df_enc = pd.get_dummies(df_sub, columns=cat_cols, drop_first=True)\n",
    "    scaler = StandardScaler()\n",
    "    df_enc[num_cols] = scaler.fit_transform(df_enc[num_cols])\n",
    "    return df_enc, scaler\n",
    "\n",
    "eco_enc, eco_scaler = encode_and_scale(eco_raw)\n",
    "biz_enc, biz_scaler = encode_and_scale(biz_raw)\n",
    "\n",
    "# æ‰¾å…±åŒç‰¹å¾µ\n",
    "common_feats = list(set(eco_enc.columns) & set(biz_enc.columns))\n",
    "for col in [log_col, true_col]:\n",
    "    if col in common_feats:\n",
    "        common_feats.remove(col)\n",
    "\n",
    "# --------------------------\n",
    "# 4. æ‹†åˆ†è¨“ç·´é›† & è¨“ç·´æ¨¡å‹\n",
    "# --------------------------\n",
    "def split_and_train(df_enc, seed=123):\n",
    "    X = df_enc[common_feats]\n",
    "    y = df_enc[log_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed\n",
    "    )\n",
    "    model = XGBRegressor(n_estimators=100, random_state=seed, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"ğŸ”¹æ¸¬è©¦é›† MSE = {mean_squared_error(y_test, y_pred):.4f}, R2 = {r2_score(y_test, y_pred):.4f}\")\n",
    "    return model, X_train, y_train, X_test\n",
    "\n",
    "eco_model, eco_X_train, eco_y_train, eco_X_test = split_and_train(eco_enc)\n",
    "biz_model, biz_X_train, biz_y_train, biz_X_test = split_and_train(biz_enc)\n",
    "\n",
    "# --------------------------\n",
    "# 5. æ¸¬è©¦é›†è³‡æ–™ï¼šæ“·å–çµ„åˆ\n",
    "# --------------------------\n",
    "def get_test_combinations(df_raw, code):\n",
    "    cols = cat_cols + num_cols + [true_col]\n",
    "    return df_raw[df_raw[group_col] == code][cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# 6. å¹³è¡Œ Bootstrap å–®æ¬¡è¨“ç·´\n",
    "# --------------------------\n",
    "def train_and_predict_bootstrap(i, X_train, y_train, X_test):\n",
    "    boot_idx = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_boot = X_train.iloc[boot_idx]\n",
    "    y_boot = y_train.iloc[boot_idx]\n",
    "    model = XGBRegressor(n_estimators=100, random_state=123+i, n_jobs=-1)\n",
    "    model.fit(X_boot, y_boot)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "# --------------------------\n",
    "# 7. é æ¸¬åƒ¹æ ¼ + é æ¸¬å€é–“ä¸¦å­˜åœ¨ä¸€èµ·\n",
    "# --------------------------\n",
    "def predict_and_save_testset_with_interval(raw_df, model, X_train, y_train, X_test, scaler, prefix, n_bootstrap=100):\n",
    "    out_dir = 'predict_data/long'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    test_index = X_test.index\n",
    "    test_df = raw_df.loc[test_index]\n",
    "    airports = sorted(test_df[group_col].unique())\n",
    "\n",
    "    # è¨ˆç®— sigmaÂ²ï¼ˆæ¸¬è©¦é›†èª¤å·®è®Šç•°æ•¸ï¼‰\n",
    "    log_preds_test = model.predict(X_test)\n",
    "    sigma2 = np.var(log_preds_test - np.log1p(test_df[true_col]))\n",
    "\n",
    "    for code in airports:\n",
    "        combos = get_test_combinations(test_df, code)\n",
    "        if combos.empty:\n",
    "            continue\n",
    "\n",
    "        # One-hot encode & scale\n",
    "        sub = combos[cat_cols + num_cols]\n",
    "        enc = pd.get_dummies(sub, columns=cat_cols, drop_first=True)\n",
    "        enc = enc.reindex(columns=common_feats, fill_value=0)\n",
    "        enc[num_cols] = scaler.transform(combos[num_cols])\n",
    "\n",
    "        # é æ¸¬å¹³å‡åƒ¹æ ¼\n",
    "        log_preds = model.predict(enc)\n",
    "        combos['é æ¸¬_å¹³å‡åƒ¹æ ¼'] = np.expm1(log_preds + 0.5 * sigma2)\n",
    "\n",
    "        # Bootstrap é æ¸¬å€é–“\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(train_and_predict_bootstrap)(i, X_train, y_train, enc)\n",
    "            for i in range(n_bootstrap)\n",
    "        )\n",
    "        y_preds_bootstrap = np.array(results)\n",
    "\n",
    "        preds_median = np.expm1(np.median(y_preds_bootstrap, axis=0))\n",
    "        preds_lower = np.expm1(np.percentile(y_preds_bootstrap, 2.5, axis=0))\n",
    "        preds_upper = np.expm1(np.percentile(y_preds_bootstrap, 97.5, axis=0))\n",
    "\n",
    "        # åŠ ä¸Šé æ¸¬å€é–“\n",
    "        combos['é æ¸¬ä¸­ä½æ•¸'] = preds_median\n",
    "        combos['é æ¸¬å€é–“ä¸‹ç•Œ'] = preds_lower\n",
    "        combos['é æ¸¬å€é–“ä¸Šç•Œ'] = preds_upper\n",
    "        combos[group_col] = code\n",
    "\n",
    "        combos = combos.drop_duplicates(subset=cat_cols + num_cols, keep='last')\n",
    "\n",
    "        # å„²å­˜\n",
    "        fn = os.path.join(out_dir, f\"{prefix}_{code}.csv\")\n",
    "        combos.to_csv(fn, index=False)\n",
    "        print(f\"âœ… é æ¸¬çµæœå·²å­˜ï¼š{fn}ï¼ˆå…± {len(combos)} ç­†ï¼‰\")\n",
    "\n",
    "# --------------------------\n",
    "# 8. åŸ·è¡Œ\n",
    "# --------------------------\n",
    "predict_and_save_testset_with_interval(eco_raw, eco_model, eco_X_train, eco_y_train, eco_X_test, eco_scaler, prefix='eco', n_bootstrap=100)\n",
    "predict_and_save_testset_with_interval(biz_raw, biz_model, biz_X_train, biz_y_train, biz_X_test, biz_scaler, prefix='biz', n_bootstrap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹[ç¶“æ¿Ÿè‰™] æ¸¬è©¦é›† MSE = 0.0267, R2 = 0.9129\n",
      "ğŸ”¹[å•†å‹™è‰™] æ¸¬è©¦é›† MSE = 0.0143, R2 = 0.8928\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_CDG.csv (1903 ç­†)\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_FRA.csv (2347 ç­†)\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_JFK.csv (1361 ç­†)\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_LAX.csv (2603 ç­†)\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_LHR.csv (2949 ç­†)\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_SYD.csv (2269 ç­†)\n",
      "âœ…[ç¶“æ¿Ÿè‰™] å·²å­˜ï¼špredict_data/long/eco_ZRH.csv (1623 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_CDG.csv (1499 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_FRA.csv (1841 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_JFK.csv (1392 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_LAX.csv (2561 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_LHR.csv (2260 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_SYD.csv (1826 ç­†)\n",
      "âœ…[å•†å‹™è‰™] å·²å­˜ï¼špredict_data/long/biz_ZRH.csv (1303 ç­†)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --------------------------\n",
    "# 1. è®€å–è³‡æ–™\n",
    "# --------------------------\n",
    "df = pd.read_csv('/Users/yuchingchen/Documents/å°ˆé¡Œ/merge_and_cleaned/final_data/long_flight.csv')\n",
    "\n",
    "# --------------------------\n",
    "# 2. æ¬„ä½å®šç¾©\n",
    "# --------------------------\n",
    "cat_cols  = [\n",
    "    'å‡ºç™¼æ™‚æ®µ','å‡ºç™¼æ©Ÿå ´ä»£è™Ÿ','æŠµé”æ™‚æ®µ',\n",
    "    'èˆªç©ºè¯ç›Ÿçµ„åˆ','èˆªç©ºå…¬å¸çµ„åˆ','èˆªç©ºè¯ç›Ÿ',\n",
    "    'åœé ç«™æ•¸é‡','æ©Ÿå‹åˆ†é¡','å‡æœŸ',\n",
    "    'é£›è¡Œæ™‚é–“å…©æ®µåˆ†é¡','æ˜¯å¦ç‚ºå¹³æ—¥'\n",
    "]\n",
    "group_col = 'æŠµé”æ©Ÿå ´ä»£è™Ÿ'\n",
    "num_cols  = [\n",
    "    'åœç•™æ™‚é–“_åˆ†é˜','å¯¦éš›é£›è¡Œæ™‚é–“_åˆ†é˜',\n",
    "    'ç¶“æ¿ŸæŒ‡æ¨™','æ©Ÿå ´æŒ‡æ¨™','competing_flights'\n",
    "]\n",
    "log_col   = 'å¹³å‡åƒ¹æ ¼_log'\n",
    "true_col  = 'å¹³å‡åƒ¹æ ¼'\n",
    "seed      = 123\n",
    "\n",
    "# --------------------------\n",
    "# 3. åˆ†è‰™ç­‰ä¸¦æ¨™è¨˜çœŸå¯¦åƒ¹æ ¼\n",
    "# --------------------------\n",
    "eco_raw = df[df['è‰™ç­‰ï¼ˆä¸»èˆªæ®µï¼‰']=='ç¶“æ¿Ÿè‰™'][cat_cols + [group_col] + num_cols + [log_col] + [true_col]].copy()\n",
    "biz_raw = df[df['è‰™ç­‰ï¼ˆä¸»èˆªæ®µï¼‰']=='å•†å‹™è‰™'][cat_cols + [group_col] + num_cols + [log_col] + [true_col]].copy()\n",
    "\n",
    "# --------------------------\n",
    "# 4. ç·¨ç¢¼èˆ‡æ¨™æº–åŒ–\n",
    "# --------------------------\n",
    "def encode_and_scale(df_raw):\n",
    "    df_enc = pd.get_dummies(df_raw, columns=cat_cols, drop_first=True)\n",
    "    scaler = StandardScaler()\n",
    "    df_enc[num_cols] = scaler.fit_transform(df_enc[num_cols])\n",
    "    return df_enc, scaler\n",
    "\n",
    "eco_enc, eco_scaler = encode_and_scale(eco_raw)\n",
    "biz_enc, biz_scaler = encode_and_scale(biz_raw)\n",
    "\n",
    "# --------------------------\n",
    "# 5. å–å…±åŒç‰¹å¾µæ¬„ä½\n",
    "# --------------------------\n",
    "common_feats = list(set(eco_enc.columns) & set(biz_enc.columns))\n",
    "for col in [log_col, true_col, group_col]:\n",
    "    if col in common_feats:\n",
    "        common_feats.remove(col)\n",
    "\n",
    "# --------------------------\n",
    "# 6. æœ€ä½³åƒæ•¸ï¼ˆé•·ç¨‹å¹³å‡åƒ¹æ ¼_logï¼Œå…¨ç”¨ XGBoostï¼‰\n",
    "# --------------------------\n",
    "best_params_avg = {\n",
    "    'ç¶“æ¿Ÿè‰™': {\n",
    "        'XGBoost': {\n",
    "            'n_estimators': 196,\n",
    "            'learning_rate': 0.1369319380373383,\n",
    "            'max_depth': 9,\n",
    "            'subsample': 0.831877718809044,\n",
    "            'colsample_bytree': 0.8877875879142252\n",
    "        }\n",
    "    },\n",
    "    'å•†å‹™è‰™': {\n",
    "        'XGBoost': {\n",
    "            'n_estimators': 196,\n",
    "            'learning_rate': 0.1369319380373383,\n",
    "            'max_depth': 9,\n",
    "            'subsample': 0.831877718809044,\n",
    "            'colsample_bytree': 0.8877875879142252\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 7. è¨“ç·´æ¨¡å‹ & æ‹†åˆ†æ¸¬è©¦é›†ï¼ˆå…¨ç”¨ XGBoostï¼‰\n",
    "# --------------------------\n",
    "def split_and_train(df_enc, cabin_type):\n",
    "    X = df_enc[common_feats]\n",
    "    y = df_enc[log_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed\n",
    "    )\n",
    "    params = best_params_avg[cabin_type]['XGBoost']\n",
    "    model = XGBRegressor(\n",
    "        tree_method='hist',\n",
    "        verbosity=0,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"ğŸ”¹[{cabin_type}] æ¸¬è©¦é›† MSE = {mean_squared_error(y_test, y_pred):.4f}, \"\n",
    "          f\"R2 = {r2_score(y_test, y_pred):.4f}\")\n",
    "    return model, X_train, y_train, X_test\n",
    "\n",
    "eco_model, eco_X_train, eco_y_train, eco_X_test = split_and_train(eco_enc, 'ç¶“æ¿Ÿè‰™')\n",
    "biz_model, biz_X_train, biz_y_train, biz_X_test = split_and_train(biz_enc, 'å•†å‹™è‰™')\n",
    "\n",
    "# --------------------------\n",
    "# 8. Bootstrap å–®æ¬¡è¨“ç·´ï¼ˆå…¨ç”¨ XGBoostï¼‰\n",
    "# --------------------------\n",
    "def train_and_predict_bootstrap(i, X_train, y_train, X_test, cabin_type):\n",
    "    idx = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    Xb, yb = X_train.iloc[idx], y_train.iloc[idx]\n",
    "    params = best_params_avg[cabin_type]['XGBoost']\n",
    "    model = XGBRegressor(\n",
    "        tree_method='hist',\n",
    "        verbosity=0,\n",
    "        random_state=seed + i,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "    model.fit(Xb, yb)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# 9. é æ¸¬ & å„²å­˜å¸¶å€é–“çµæœ\n",
    "# --------------------------\n",
    "def predict_and_save_with_interval(\n",
    "    raw_df, model, X_train, y_train, X_test, scaler, cabin_type, prefix, n_boot=100\n",
    "):\n",
    "    out_dir = f'predict_data/long'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    idx_test = X_test.index\n",
    "    test_df  = raw_df.loc[idx_test]\n",
    "    sigma2   = np.var(model.predict(X_test) - np.log1p(test_df[true_col]))\n",
    "\n",
    "    for code in sorted(test_df[group_col].unique()):\n",
    "        combo = test_df[test_df[group_col]==code][cat_cols+num_cols+[true_col]] \\\n",
    "                .drop_duplicates().reset_index(drop=True)\n",
    "        if combo.empty: continue\n",
    "\n",
    "        enc = pd.get_dummies(combo, columns=cat_cols, drop_first=True)\n",
    "        enc = enc.reindex(columns=common_feats, fill_value=0)\n",
    "        enc[num_cols] = scaler.transform(combo[num_cols])\n",
    "\n",
    "        lp = model.predict(enc)\n",
    "        combo['é æ¸¬_å¹³å‡åƒ¹æ ¼'] = np.expm1(lp + 0.5*sigma2)\n",
    "\n",
    "        boots = Parallel(n_jobs=-1)(\n",
    "            delayed(train_and_predict_bootstrap)(\n",
    "                i, X_train, y_train, enc, cabin_type\n",
    "            ) for i in range(n_boot)\n",
    "        )\n",
    "        arr = np.array(boots)\n",
    "\n",
    "        combo['é æ¸¬ä¸­ä½æ•¸']   = np.expm1(np.median(arr, axis=0))\n",
    "        combo['é æ¸¬å€é–“ä¸‹ç•Œ'] = np.expm1(np.percentile(arr,2.5,axis=0))\n",
    "        combo['é æ¸¬å€é–“ä¸Šç•Œ'] = np.expm1(np.percentile(arr,97.5,axis=0))\n",
    "        combo[group_col] = code\n",
    "\n",
    "        fn = f'{out_dir}/{prefix}_{code}.csv'\n",
    "        combo.to_csv(fn, index=False)\n",
    "        print(f\"âœ…[{cabin_type}] å·²å­˜ï¼š{fn} ({len(combo)} ç­†)\")\n",
    "\n",
    "predict_and_save_with_interval(\n",
    "    eco_raw, eco_model, eco_X_train, eco_y_train, eco_X_test,\n",
    "    eco_scaler, 'ç¶“æ¿Ÿè‰™', 'eco', n_boot=100\n",
    ")\n",
    "predict_and_save_with_interval(\n",
    "    biz_raw, biz_model, biz_X_train, biz_y_train, biz_X_test,\n",
    "    biz_scaler, 'å•†å‹™è‰™', 'biz', n_boot=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
